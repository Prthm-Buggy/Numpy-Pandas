{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3f18d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8a0371a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Amazon.in for multiple categories using BeautifulSoup...\n",
      "\n",
      "\n",
      "Scraping category: laptop\n",
      "Scraping: https://www.amazon.in/s?k=laptop&page=1\n",
      "Scraping: https://www.amazon.in/s?k=laptop&page=2\n",
      "Scraping: https://www.amazon.in/s?k=laptop&page=3\n",
      "\n",
      "Scraping category: smartphone\n",
      "Scraping: https://www.amazon.in/s?k=smartphone&page=1\n",
      "Scraping: https://www.amazon.in/s?k=smartphone&page=2\n",
      "Scraping: https://www.amazon.in/s?k=smartphone&page=3\n",
      "\n",
      "Scraping category: headphones\n",
      "Scraping: https://www.amazon.in/s?k=headphones&page=1\n",
      "Scraping: https://www.amazon.in/s?k=headphones&page=2\n",
      "Scraping: https://www.amazon.in/s?k=headphones&page=3\n",
      "\n",
      "Scraping category: books\n",
      "Scraping: https://www.amazon.in/s?k=books&page=1\n",
      "Scraping: https://www.amazon.in/s?k=books&page=2\n",
      "Scraping: https://www.amazon.in/s?k=books&page=3\n",
      "Saved 284 products to amazon_products_bs4.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "# ---------- CONFIGURABLE PART ----------\n",
    "SEARCH_QUERIES = [\"laptop\", \"smartphone\", \"headphones\", \"books\"]\n",
    "BASE_URL = \"https://www.amazon.in\"\n",
    "MAX_PAGES = 3  # Number of pages to scrape per category\n",
    "OUTPUT_FILE = \"amazon_products_bs4.csv\"\n",
    "\n",
    "HEADERS_LIST = [\n",
    "    {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\"\n",
    "    },\n",
    "    {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.8\"\n",
    "    },\n",
    "    {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64)\",\n",
    "        \"Accept-Language\": \"en-GB,en;q=0.7\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# ---------- FUNCTION TO SCRAPE ONE PAGE ----------\n",
    "def scrape_page(url, category):\n",
    "    print(f\"Scraping: {url}\")\n",
    "    headers = random.choice(HEADERS_LIST)\n",
    "    response = requests.get(url, headers=headers)\n",
    "    time.sleep(random.uniform(2, 4))\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch page: {url}, Status Code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "    items = soup.select(\"div.s-main-slot div[data-component-type='s-search-result']\")\n",
    "    product_list = []\n",
    "\n",
    "    for item in items:\n",
    "        title_tag = item.select_one(\"h2 a\")\n",
    "        title = title_tag.get_text(strip=True) if title_tag else None\n",
    "        link = \"https://www.amazon.in\" + title_tag['href'] if title_tag and title_tag.has_attr('href') else None\n",
    "\n",
    "        price_whole = item.select_one(\"span.a-price-whole\")\n",
    "        price_fraction = item.select_one(\"span.a-price-fraction\")\n",
    "        price = f\"{price_whole.text.strip().replace(',', '')}.{price_fraction.text.strip()}\" if price_whole and price_fraction else None\n",
    "\n",
    "        rating_tag = item.select_one(\"span.a-icon-alt\")\n",
    "        rating = rating_tag.get_text(strip=True) if rating_tag else None\n",
    "\n",
    "        reviews_tag = item.select_one(\"span.a-size-base\")\n",
    "        reviews = reviews_tag.get_text(strip=True) if reviews_tag else None\n",
    "\n",
    "        badge_tag = item.select_one(\"span.s-label-popover-default\")\n",
    "        badge = badge_tag.get_text(strip=True) if badge_tag else None\n",
    "\n",
    "        image_tag = item.select_one(\"img.s-image\")\n",
    "        image_url = image_tag['src'] if image_tag and image_tag.has_attr('src') else None\n",
    "\n",
    "        brand_tag = item.select_one(\"span.a-size-base-plus.a-color-base\")\n",
    "        brand = brand_tag.get_text(strip=True) if brand_tag else None\n",
    "\n",
    "        delivery_tag = item.select_one(\"span.a-color-base.a-text-bold\")\n",
    "        delivery = delivery_tag.get_text(strip=True) if delivery_tag else None\n",
    "\n",
    "        discount_tag = item.select_one(\"span.a-letter-space + span.a-size-base.a-color-secondary\")\n",
    "        discount = discount_tag.get_text(strip=True) if discount_tag else None\n",
    "\n",
    "        product = {\n",
    "            \"Category\": category,\n",
    "            \"Title\": title,\n",
    "            \"Brand\": brand,\n",
    "            \"Price (INR)\": price,\n",
    "            \"Rating\": rating,\n",
    "            \"Review Count\": reviews,\n",
    "            \"Product Badge\": badge,\n",
    "            \"Product URL\": link,\n",
    "            \"Image URL\": image_url,\n",
    "            \"Delivery Info\": delivery,\n",
    "            \"Discount\": discount\n",
    "        }\n",
    "        product_list.append(product)\n",
    "\n",
    "    return product_list\n",
    "\n",
    "# ---------- MAIN SCRAPER LOOP ----------\n",
    "def scrape_amazon(queries, max_pages=1):\n",
    "    all_products = []\n",
    "\n",
    "    for query in queries:\n",
    "        print(f\"\\nScraping category: {query}\")\n",
    "        page = 1\n",
    "\n",
    "        while page <= max_pages:\n",
    "            search_url = f\"{BASE_URL}/s?k={quote_plus(query)}&page={page}\"\n",
    "            products = scrape_page(search_url, query)\n",
    "\n",
    "            if not products:\n",
    "                print(\"No products found or blocked. Stopping.\")\n",
    "                break\n",
    "\n",
    "            all_products.extend(products)\n",
    "            page += 1\n",
    "\n",
    "    return all_products\n",
    "\n",
    "# ---------- SAVE RESULTS ----------\n",
    "def save_to_csv(products, filename):\n",
    "    df = pd.DataFrame(products)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved {len(products)} products to {filename}\")\n",
    "\n",
    "# ---------- RUNNING THE SCRAPER ----------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Scraping Amazon.in for multiple categories using BeautifulSoup...\\n\")\n",
    "    data = scrape_amazon(SEARCH_QUERIES, MAX_PAGES)\n",
    "    save_to_csv(data, OUTPUT_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6241f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
