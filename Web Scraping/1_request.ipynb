{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eb19ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install requests\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ba5d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://youtube.com/watch?v=9bZkp7q19f0'  # Example URL, replace with the desired one\n",
    "response = requests.get(url)\n",
    "# Check if the request was successful\n",
    "print(\"successful\" if response.status_code == 200 else \"Page not found\" if response.status_code == 404 else \"Something went wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aee4b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.raise_for_status()  # Throws error for bad requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac09a904",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.github.com/users/octocat\"\n",
    "response = requests.get(url)\n",
    "\n",
    "data = response.json()\n",
    "print(data)\n",
    "print(data['name'])        # The Octocat\n",
    "print(data['public_repos'])  # e.g., 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5797eb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://pokeapi.co/api/v2\"  # Example URL for a Pokémon API\n",
    "response = requests.get(url)\n",
    "\n",
    "def pokemon_info(name):\n",
    "    url = f\"https://pokeapi.co/api/v2/pokemon/{name}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "pokemon_name = \"raichu\"  # Example Pokémon name, replace with the desired one\n",
    "response = requests.get(f\"https://pokeapi.co/api/v2/pokemon/{pokemon_name}\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    print(f'Name:', data['name'])\n",
    "    print(f'Height:', data['height'])\n",
    "    print(f'Weight:', data['weight'])\n",
    "    print(f'Id:', data['id'])\n",
    "    print(f\"Pokemon data extracted successfully!\")\n",
    "else:\n",
    "    print(f\"Error: Pokémon '{pokemon_name}' not found (status code {response.status_code})\")\n",
    "    print(f'failed to extract Pokémon data for {pokemon_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb107cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://in.pinterest.com/pin/44050902599444305/'\n",
    "response = requests.get(url)\n",
    "\n",
    "with open('image.png', 'wb') as f:\n",
    "    f.write(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1937c962",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.headers)\n",
    "print(response.cookies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7664940c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "response = requests.get('https://httpbin.org/basic-auth/user/pass',\n",
    "                        auth=HTTPBasicAuth('user', 'pass'))\n",
    "print(response.status_code)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7ef159",
   "metadata": {},
   "source": [
    " 1. requests.get()\n",
    "Used to get (fetch) data from a URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240f28a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get('https://api.github.com')\n",
    "\n",
    "print(response.status_code)   # HTTP status code, like 200 (OK)\n",
    "print(response.text)          # Response content as a string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a21749",
   "metadata": {},
   "source": [
    "2. requests.post()\n",
    "Used to send data (like login info, forms) to a server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9155ec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"name\":\"pratham\", \"age\": 20}\n",
    "response  = requests.post('https://httpbin.org/post', json=data)\n",
    "print(data)\n",
    "print()\n",
    "print(f'The connection is successful {response.status_code}' if response.status_code == 200 else 'failed')\n",
    "print(response.json()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80711004",
   "metadata": {},
   "source": [
    " 3. requests.put()\n",
    "Used to update existing data on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724be7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'name': 'Prathamesh Talele'}\n",
    "\n",
    "response = requests.put('https://httpbin.org/put', data=data)\n",
    "\n",
    "print(response.status_code)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea2c57e",
   "metadata": {},
   "source": [
    "4. requests.delete()\n",
    "Used to delete data on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b1c5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "respone = requests.delete('https://httpbin.org/delete' )\n",
    "\n",
    "print(f'The connection is successful {respone.status_code}' if respone.status_code == 200 else \"failed\")\n",
    "print(respone.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8240c821",
   "metadata": {},
   "source": [
    " 5. requests.head()\n",
    "Sends a request just for the headers (not the content). Useful to check if a page exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adf7d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Date': 'Mon, 07 Jul 2025 06:38:55 GMT', 'Content-Type': 'application/json', 'Content-Length': '307', 'Connection': 'keep-alive', 'Server': 'gunicorn/19.9.0', 'Access-Control-Allow-Origin': '*', 'Access-Control-Allow-Credentials': 'true'}\n"
     ]
    }
   ],
   "source": [
    "response = requests.head('https://httpbin.org/get')\n",
    "print(response.headers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b81425",
   "metadata": {},
   "source": [
    "6. requests.options()\n",
    "Used to find out what methods are allowed on a server (like GET, POST, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e6bfea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET, OPTIONS, HEAD\n"
     ]
    }
   ],
   "source": [
    "response = requests.options('https://httpbin.org')\n",
    "print(response.headers['Allow'])  # Example header, replace with the desired one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6ae21c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://httpbin.org/get?search=python\n"
     ]
    }
   ],
   "source": [
    "params = {'search': 'python'}\n",
    "\n",
    "response = requests.get('https://httpbin.org/get', params=params)\n",
    "\n",
    "print(response.url)  # See the full URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "158cb67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n",
      "{\n",
      "  \"args\": {}, \n",
      "  \"data\": \"\", \n",
      "  \"files\": {}, \n",
      "  \"form\": {\n",
      "    \"username\": \"pratham\"\n",
      "  }, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*\", \n",
      "    \"Accept-Encoding\": \"gzip, deflate\", \n",
      "    \"Content-Length\": \"16\", \n",
      "    \"Content-Type\": \"application/x-www-form-urlencoded\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"myapp/1.0\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-686b6ca8-311303dd2764fb3742ad3cdd\"\n",
      "  }, \n",
      "  \"json\": null, \n",
      "  \"origin\": \"103.87.31.207\", \n",
      "  \"url\": \"https://httpbin.org/post\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = \"https://httpbin.org/post\"\n",
    "headers = {\"User-Agent\": \"myapp/1.0\"}\n",
    "data = {\"username\": \"pratham\"}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=data)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Success!\")\n",
    "    print(response.text)\n",
    "else:\n",
    "    print(\"Failed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aa61569b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Amazon.in for 'laptop'...\n",
      "\n",
      "Scraping: https://www.amazon.in/s?k=laptop&page=1\n",
      "Scraping: https://www.amazon.in/s?k=laptop&page=2\n",
      "Scraping: https://www.amazon.in/s?k=laptop&page=3\n",
      "Scraping: https://www.amazon.in/s?k=laptop&page=4\n",
      "Scraping: https://www.amazon.in/s?k=laptop&page=5\n",
      "Saved 98 products to amazon_laptops_advanced.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "# ---------- CONFIGURABLE PART ----------\n",
    "SEARCH_QUERY = \"laptop\"\n",
    "BASE_URL = \"https://www.amazon.in\"\n",
    "MAX_PAGES = 5  # Number of pages to scrape\n",
    "OUTPUT_FILE = \"amazon_laptops_advanced.csv\"\n",
    "\n",
    "HEADERS_LIST = [\n",
    "    {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\"\n",
    "    },\n",
    "    {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.8\"\n",
    "    },\n",
    "    {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64)\",\n",
    "        \"Accept-Language\": \"en-GB,en;q=0.7\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# ---------- FUNCTION TO SCRAPE ONE PAGE ----------\n",
    "def scrape_page(url):\n",
    "    print(f\"Scraping: {url}\")\n",
    "    headers = random.choice(HEADERS_LIST)\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch page: {url}, Status Code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "    items = soup.select(\"div.s-main-slot div[data-component-type='s-search-result']\")\n",
    "\n",
    "    product_list = []\n",
    "\n",
    "    for item in items:\n",
    "        title = item.h2.text.strip() if item.h2 else None\n",
    "        link = BASE_URL + item.h2.a['href'] if item.h2 and item.h2.a else None\n",
    "\n",
    "        price_whole = item.select_one(\"span.a-price-whole\")\n",
    "        price = price_whole.text.strip().replace(\",\", \"\") if price_whole else None\n",
    "\n",
    "        rating = item.select_one(\"span.a-icon-alt\")\n",
    "        rating = rating.text.strip() if rating else None\n",
    "\n",
    "        reviews = item.select_one(\"span.a-size-base\")\n",
    "        reviews = reviews.text.strip() if reviews else None\n",
    "\n",
    "        product = {\n",
    "            \"Title\": title,\n",
    "            \"Price (INR)\": price,\n",
    "            \"Rating\": rating,\n",
    "            \"Review Count\": reviews,\n",
    "            \"Product URL\": link\n",
    "        }\n",
    "        product_list.append(product)\n",
    "\n",
    "    return product_list\n",
    "\n",
    "# ---------- MAIN SCRAPER LOOP ----------\n",
    "def scrape_amazon(query, max_pages=1):\n",
    "    all_products = []\n",
    "    page = 1\n",
    "\n",
    "    while page <= max_pages:\n",
    "        search_url = f\"{BASE_URL}/s?k={quote_plus(query)}&page={page}\"\n",
    "        products = scrape_page(search_url)\n",
    "\n",
    "        if not products:\n",
    "            print(\"No products found or blocked. Stopping.\")\n",
    "            break\n",
    "\n",
    "        all_products.extend(products)\n",
    "\n",
    "        # Random sleep to avoid blocking\n",
    "        time.sleep(random.uniform(2, 5))\n",
    "        page += 1\n",
    "\n",
    "    return all_products\n",
    "\n",
    "# ---------- SAVE RESULTS ----------\n",
    "def save_to_csv(products, filename):\n",
    "    df = pd.DataFrame(products)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved {len(products)} products to {filename}\")\n",
    "\n",
    "# ---------- RUNNING THE SCRAPER ----------\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Scraping Amazon.in for '{SEARCH_QUERY}'...\\n\")\n",
    "    data = scrape_amazon(SEARCH_QUERY, MAX_PAGES)\n",
    "    save_to_csv(data, OUTPUT_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae95788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Amazon.in for multiple categories...\n",
      "\n",
      "\n",
      "Scraping category: laptop\n",
      "Scraping: https://www.amazon.in/s?k=laptop&page=1\n",
      "Scraping: https://www.amazon.in/s?k=laptop&page=2\n",
      "Scraping: https://www.amazon.in/s?k=laptop&page=3\n",
      "\n",
      "Scraping category: smartphone\n",
      "Scraping: https://www.amazon.in/s?k=smartphone&page=1\n",
      "Scraping: https://www.amazon.in/s?k=smartphone&page=2\n",
      "Scraping: https://www.amazon.in/s?k=smartphone&page=3\n",
      "\n",
      "Scraping category: headphones\n",
      "Scraping: https://www.amazon.in/s?k=headphones&page=1\n",
      "Scraping: https://www.amazon.in/s?k=headphones&page=2\n",
      "Scraping: https://www.amazon.in/s?k=headphones&page=3\n",
      "\n",
      "Scraping category: books\n",
      "Scraping: https://www.amazon.in/s?k=books&page=1\n",
      "Scraping: https://www.amazon.in/s?k=books&page=2\n",
      "Scraping: https://www.amazon.in/s?k=books&page=3\n",
      "Saved 234 products to amazon_products_multi_category.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "# ---------- CONFIGURABLE PART ----------\n",
    "SEARCH_QUERIES = [\"laptop\", \"smartphone\", \"headphones\", \"books\"]\n",
    "BASE_URL = \"https://www.amazon.in\"\n",
    "MAX_PAGES = 3  # Number of pages to scrape per category\n",
    "OUTPUT_FILE = \"amazon_products.csv\"\n",
    "\n",
    "HEADERS_LIST = [\n",
    "    {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\"\n",
    "    },\n",
    "    {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.8\"\n",
    "    },\n",
    "    {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64)\",\n",
    "        \"Accept-Language\": \"en-GB,en;q=0.7\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# ---------- FUNCTION TO SCRAPE ONE PAGE ----------\n",
    "def scrape_page(url, category):\n",
    "    print(f\"Scraping: {url}\")\n",
    "    headers = random.choice(HEADERS_LIST)\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch page: {url}, Status Code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "    items = soup.select(\"div.s-main-slot div[data-component-type='s-search-result']\")\n",
    "\n",
    "    product_list = []\n",
    "\n",
    "    for item in items:\n",
    "        title = item.h2.text.strip() if item.h2 else None\n",
    "        link = BASE_URL + item.h2.a['href'] if item.h2 and item.h2.a else None\n",
    "\n",
    "        price_whole = item.select_one(\"span.a-price-whole\")\n",
    "        price_fraction = item.select_one(\"span.a-price-fraction\")\n",
    "        price = f\"{price_whole.text.strip().replace(',', '')}.{price_fraction.text.strip()}\" if price_whole and price_fraction else None\n",
    "\n",
    "        rating = item.select_one(\"span.a-icon-alt\")\n",
    "        rating = rating.text.strip() if rating else None\n",
    "\n",
    "        reviews = item.select_one(\"span.a-size-base\")\n",
    "        reviews = reviews.text.strip() if reviews else None\n",
    "\n",
    "        badge = item.select_one(\"span.s-label-popover-default\")\n",
    "        badge_text = badge.text.strip() if badge else None\n",
    "\n",
    "        image = item.select_one(\"img.s-image\")\n",
    "        image_url = image[\"src\"] if image else None\n",
    "\n",
    "        brand = item.select_one(\"span.a-size-base-plus.a-color-base\")\n",
    "        brand_name = brand.text.strip() if brand else None\n",
    "\n",
    "        delivery = item.select_one(\"span.a-color-base.a-text-bold\")\n",
    "        delivery_info = delivery.text.strip() if delivery else None\n",
    "\n",
    "        discount = item.select_one(\"span.a-letter-space + span.a-size-base.a-color-secondary\")\n",
    "        discount_text = discount.text.strip() if discount else None\n",
    "\n",
    "        product = {\n",
    "            \"Category\": category,\n",
    "            \"Title\": title,\n",
    "            \"Brand\": brand_name,\n",
    "            \"Price (INR)\": price,\n",
    "            \"Rating\": rating,\n",
    "            \"Review Count\": reviews,\n",
    "            \"Product Badge\": badge_text,\n",
    "            \"Product URL\": link,\n",
    "            \"Image URL\": image_url,\n",
    "            \"Delivery Info\": delivery_info,\n",
    "            \"Discount\": discount_text\n",
    "        }\n",
    "        product_list.append(product)\n",
    "\n",
    "    return product_list\n",
    "\n",
    "# ---------- MAIN SCRAPER LOOP ----------\n",
    "def scrape_amazon(queries, max_pages=1):\n",
    "    all_products = []\n",
    "\n",
    "    for query in queries:\n",
    "        print(f\"\\nScraping category: {query}\")\n",
    "        page = 1\n",
    "\n",
    "        while page <= max_pages:\n",
    "            search_url = f\"{BASE_URL}/s?k={quote_plus(query)}&page={page}\"\n",
    "            products = scrape_page(search_url, query)\n",
    "\n",
    "            if not products:\n",
    "                print(\"No products found or blocked. Stopping.\")\n",
    "                break\n",
    "\n",
    "            all_products.extend(products)\n",
    "\n",
    "            time.sleep(random.uniform(2, 5))\n",
    "            page += 1\n",
    "\n",
    "    return all_products\n",
    "\n",
    "# ---------- SAVE RESULTS ----------\n",
    "def save_to_csv(products, filename):\n",
    "    df = pd.DataFrame(products)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved {len(products)} products to {filename}\")\n",
    "\n",
    "# ---------- RUNNING THE SCRAPER ----------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Scraping Amazon.in for multiple categories...\\n\")\n",
    "    data = scrape_amazon(SEARCH_QUERIES, MAX_PAGES)\n",
    "    save_to_csv(data, OUTPUT_FILE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
