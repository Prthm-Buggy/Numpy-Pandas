{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36eb19ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\prath\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\prath\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prath\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\prath\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prath\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests) (2025.6.15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install requests\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33ba5d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successful\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://youtube.com/watch?v=9bZkp7q19f0'  # Example URL, replace with the desired one\n",
    "response = requests.get(url)\n",
    "# Check if the request was successful\n",
    "print(\"successful\" if response.status_code == 200 else \"Page not found\" if response.status_code == 404 else \"Something went wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aee4b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.raise_for_status()  # Throws error for bad requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac09a904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'login': 'octocat', 'id': 583231, 'node_id': 'MDQ6VXNlcjU4MzIzMQ==', 'avatar_url': 'https://avatars.githubusercontent.com/u/583231?v=4', 'gravatar_id': '', 'url': 'https://api.github.com/users/octocat', 'html_url': 'https://github.com/octocat', 'followers_url': 'https://api.github.com/users/octocat/followers', 'following_url': 'https://api.github.com/users/octocat/following{/other_user}', 'gists_url': 'https://api.github.com/users/octocat/gists{/gist_id}', 'starred_url': 'https://api.github.com/users/octocat/starred{/owner}{/repo}', 'subscriptions_url': 'https://api.github.com/users/octocat/subscriptions', 'organizations_url': 'https://api.github.com/users/octocat/orgs', 'repos_url': 'https://api.github.com/users/octocat/repos', 'events_url': 'https://api.github.com/users/octocat/events{/privacy}', 'received_events_url': 'https://api.github.com/users/octocat/received_events', 'type': 'User', 'user_view_type': 'public', 'site_admin': False, 'name': 'The Octocat', 'company': '@github', 'blog': 'https://github.blog', 'location': 'San Francisco', 'email': None, 'hireable': None, 'bio': None, 'twitter_username': None, 'public_repos': 8, 'public_gists': 8, 'followers': 18564, 'following': 9, 'created_at': '2011-01-25T18:44:36Z', 'updated_at': '2025-06-22T11:21:58Z'}\n",
      "The Octocat\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "url = \"https://api.github.com/users/octocat\"\n",
    "response = requests.get(url)\n",
    "\n",
    "data = response.json()\n",
    "print(data)\n",
    "print(data['name'])        # The Octocat\n",
    "print(data['public_repos'])  # e.g., 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5797eb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: raichu\n",
      "Height: 8\n",
      "Weight: 300\n",
      "Id: 26\n",
      "Pokemon data extracted successfully!\n"
     ]
    }
   ],
   "source": [
    "url = \"https://pokeapi.co/api/v2\"  # Example URL for a Pokémon API\n",
    "response = requests.get(url)\n",
    "\n",
    "def pokemon_info(name):\n",
    "    url = f\"https://pokeapi.co/api/v2/pokemon/{name}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "pokemon_name = \"raichu\"  # Example Pokémon name, replace with the desired one\n",
    "response = requests.get(f\"https://pokeapi.co/api/v2/pokemon/{pokemon_name}\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    print(f'Name:', data['name'])\n",
    "    print(f'Height:', data['height'])\n",
    "    print(f'Weight:', data['weight'])\n",
    "    print(f'Id:', data['id'])\n",
    "    print(f\"Pokemon data extracted successfully!\")\n",
    "else:\n",
    "    print(f\"Error: Pokémon '{pokemon_name}' not found (status code {response.status_code})\")\n",
    "    print(f'failed to extract Pokémon data for {pokemon_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb107cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://in.pinterest.com/pin/44050902599444305/'\n",
    "response = requests.get(url)\n",
    "\n",
    "with open('image.png', 'wb') as f:\n",
    "    f.write(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1937c962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x-xss-protection': '1; mode=block', 'x-content-type-options': 'nosniff', 'Vary': 'User-Agent, Accept-Encoding', 'x-ua-compatible': 'IE=edge', 'reporting-endpoints': 'coop-endpoint=\"https://www.pinterest.com/_/_/coop_report/\", coep-endpoint=\"https://www.pinterest.com/_/_/coep_report/\"', 'cross-origin-opener-policy-report-only': 'same-origin; report-to=\"coop-endpoint\"', 'p3p': 'CP=\"This is not a P3P policy. See https://www.pinterest.com/_/_/help/articles/pinterest-and-p3p for more info.\"', 'Content-Security-Policy': \"default-src 'self' blob: s.pinimg.com; font-src 'self' m.media-amazon.com *.amazon-adsystem.com s.pinimg.com data: fonts.googleapis.com fonts.gstatic.com use.typekit.net; style-src 'self' blob: 'unsafe-inline' data: *.pinimg.com *.pinterest.com accounts.google.com fonts.googleapis.com *.adyen.com *.adyenpayments.com; img-src blob: data: *; base-uri 'none'; connect-src 'self' blob: m.media-amazon.com *.amazon-adsystem.com htlb.casalemedia.com *.ada.support *.pinimg.com *.pinterest.com accounts.google.com *.adyen.com pinterest-salvador.s3.amazonaws.com *.adyenpayments.com *.facebook.com www.recaptcha.net www.googleapis.com *.dropboxapi.com pinterest-aberdeen.s3.amazonaws.com pinterest-aberdeen.s3.us-east-1.amazonaws.com pinterest-anaheim.s3.amazonaws.com pinterest-anaheim.s3.us-east-1.amazonaws.com pinterest-hilo.s3.amazonaws.com pinterest-hilo.s3.us-east-1.amazonaws.com pinterest-sim-toontown.s3.amazonaws.com pinterest-sim-toontown.s3.us-east-1.amazonaws.com pinterest-media-upload.s3.amazonaws.com pinterest-media-upload.s3.us-east-1.amazonaws.com pinterest-media-upload.s3-accelerate.amazonaws.com pinterest-media-upload.s3-accelerate.us-east-1.amazonaws.com pinterest-milwaukee.s3.amazonaws.com pinterest-milwaukee.s3.us-east-1.amazonaws.com pinterest-poughkeepsie.s3.amazonaws.com pinterest-poughkeepsie.s3.us-east-1.amazonaws.com pinterest-waterloo.s3.amazonaws.com pinterest-waterloo.s3.us-east-1.amazonaws.com pinterest-plymouth.s3.amazonaws.com pinterest-plymouth.s3.us-east-1.amazonaws.com pinterest-salvador.s3.us-east-1.amazonaws.com pinterest-yamagata.s3.amazonaws.com pinterest-yamagata.s3.us-east-1.amazonaws.com *.cedexis.com *.cedexis-radar.net *.tvpixel.com api.pinadmin.com *.live-video.net https://*.daily.co https://*.pluot.blue wss://*.wss.daily.co api.basistheory.com; form-action 'self' *.adyen.com *.klarna.com *.tink.com *.adyenpayments.com; frame-src 'self' *.ada.support *.pinimg.com *.pinterest.com *.adyen.com static-sandbox.dlocal.com static.dlocal.com *.google.com *.facebook.com www.recaptcha.net pinterest-hilo.s3.amazonaws.com pinterest-hilo.s3.us-east-1.amazonaws.com pinterest-sim-toontown.s3.amazonaws.com pinterest-sim-toontown.s3.us-east-1.amazonaws.com pinterest-milwaukee.s3.amazonaws.com pinterest-milwaukee.s3.us-east-1.amazonaws.com pinterest-waterloo.s3.amazonaws.com pinterest-waterloo.s3.us-east-1.amazonaws.com pinterest-tolu.s3.amazonaws.com *.pinterdev.com content.googleapis.com *.youtube.com *.youtube-nocookie.com *.ytimg.com player.vimeo.com calendly.com vine.co bid.g.doubleclick.net *.fls.doubleclick.net pinlogs.s3.amazonaws.com pinlogs.s3.us-east-1.amazonaws.com advertising-delivery-metric-reports.s3.amazonaws.com advertising-delivery-metric-reports.s3.us-east-1.amazonaws.com servedby.flashtalking.com pinterest-uk.admo.tv pinterest-uk-web.admo.tv fbrpc://call *.linkedin.com px.ads.linkedin.com api.basistheory.com js.basistheory.com 3ds.basistheory.com; media-src 'self' blob: m.media-amazon.com data: *.pinimg.com *.live-video.net; object-src 'self'; script-src 'nonce-1d6319c2aba079ede833e2c0647d83ab' 'strict-dynamic' 'self' blob: 'unsafe-inline' *.pinimg.com *.pinterest.com *.adyen.com js.dlocal.com js-sandbox.dlocal.com static-sandbox.dlocal.com static.dlocal.com *.adyenpayments.com 'report-sample' *.google.com connect.facebook.net *.google-analytics.com *.facebook.com *.googleadservices.com *.doubleclick.net *.googletagmanager.com radar.cedexis.com *.cedexis-test.com www.gstatic.com/recaptcha/ www.gstatic.cn/recaptcha/ www.recaptcha.net 'wasm-unsafe-eval' js.basistheory.com 3ds.basistheory.com; worker-src 'self' blob: 'unsafe-inline'; report-uri /_/_/csp_report/?rid=1184130450153869; frame-ancestors 'self' , script-src 'self' blob: 'unsafe-inline' *.pinimg.com *.pinterest.com *.adyen.com js.dlocal.com js-sandbox.dlocal.com static-sandbox.dlocal.com static.dlocal.com *.adyenpayments.com 'report-sample' *.google.com connect.facebook.net *.google-analytics.com *.facebook.com *.googleadservices.com *.doubleclick.net *.googletagmanager.com radar.cedexis.com *.cedexis-test.com www.gstatic.com/recaptcha/ www.gstatic.cn/recaptcha/ www.recaptcha.net 'wasm-unsafe-eval' js.basistheory.com 3ds.basistheory.com; report-uri /_/_/csp_report/?rid=1184130450153869\", 'x-frame-options': 'SAMEORIGIN', 'origin-trial': 'AvlUIFJouPpJAKljRGh7EnYm2Brnx/eu51h39Z7p11vbzNlw2YhkUhxvxZdkS709VlGGNw4Gcg/a9mAzHDrEcQ0AAAB5eyJvcmlnaW4iOiJodHRwczovL3BpbnRlcmVzdC5jb206NDQzIiwiZmVhdHVyZSI6IlNlbmRGdWxsVXNlckFnZW50QWZ0ZXJSZWR1Y3Rpb24iLCJleHBpcnkiOjE2ODQ4ODYzOTksImlzU3ViZG9tYWluIjp0cnVlfQ==', 'Accept-CH': 'Sec-CH-UA-Full,Sec-CH-UA-Full-Version-List,Sec-CH-UA-Model,Sec-CH-UA-Platform-Version', 'Content-Type': 'text/html; charset=utf-8', 'x-async-render': 'true', 'Link': '<https://i.pinimg.com>; rel=preconnect; crossorigin=anonymous, <https://s.pinimg.com>; rel=preconnect; crossorigin=anonymous, <https://v1.pinimg.com>; rel=preconnect; crossorigin=anonymous,<https://www.pinterest.com/oembed.json?url=https%3A%2F%2Fwww.pinterest.com%2Fpin%2F44050902599444305&ref=oembed-discovery>; rel=\"alternate\"; type=\"application/json+oembed\" title=\"Find inspiration on Pinterest today!\"', 'Trailer': 'x-pinterest-sli-streamed-response-type', 'x-envoy-upstream-service-time': '253', 'pinterest-generated-by': 'coreapp-webapp-next-0a0187e1', 'Content-Encoding': 'gzip', 'pinterest-version': '76d9ff6', 'referrer-policy': 'origin', 'x-pinterest-rid': '1184130450153869', 'x-pinterest-rid-128bit': '1ea322e8008bd87da454c3b4cb72b296', 'Date': 'Tue, 08 Jul 2025 05:22:19 GMT', 'Alt-Svc': 'h3=\":443\"; ma=604800', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive, Transfer-Encoding', 'Set-Cookie': 'csrftoken=ed6522133f94f5ae59314bca7f2d5165; path=/; expires=Wed, 08 Jul 2026 05:22:18 GMT; samesite=lax; secure, _pinterest_sess=TWc9PSZTRElKVkZvTnNZQ1Eva3NNaUNSQlJZa00rWXVJOUJpZUF0ZzZaRTBzS1hCbHVhU1Ixb1ZoaGUwM2RGY1VVTjRKU1VNYkxnSVBrdHo5RmNYU1RHYkhrRGJwY2RXdDFpUDgydkFZK2RRc1dLOD0mMDE3bXNGZklMYi9nMUplbFpuVk90UUJFVTg0PQ==; path=/; expires=Fri, 03 Jul 2026 05:22:18 GMT; domain=.pinterest.com; samesite=none; secure; httponly, _auth=0; path=/; expires=Fri, 03 Jul 2026 05:22:18 GMT; domain=.pinterest.com; secure; httponly, _routing_id=\"f009b563-b239-4554-9438-cf11d0b6802d\"; Max-Age=86400; Path=/; HttpOnly', 'AKAMAI-GRN': '0.edf43717.1751952138.8796e326', 'X-CDN': 'akamai', 'Strict-Transport-Security': 'max-age=31536000 ; includeSubDomains ; preload'}\n",
      "<RequestsCookieJar[<Cookie csrftoken=ed6522133f94f5ae59314bca7f2d5165 for in.pinterest.com/>, <Cookie _routing_id=\"f009b563-b239-4554-9438-cf11d0b6802d\" for in.pinterest.com/>, <Cookie _pinterest_sess=TWc9PSZTRElKVkZvTnNZQ1Eva3NNaUNSQlJZa00rWXVJOUJpZUF0ZzZaRTBzS1hCbHVhU1Ixb1ZoaGUwM2RGY1VVTjRKU1VNYkxnSVBrdHo5RmNYU1RHYkhrRGJwY2RXdDFpUDgydkFZK2RRc1dLOD0mMDE3bXNGZklMYi9nMUplbFpuVk90UUJFVTg0PQ== for .pinterest.com/>, <Cookie _auth=0 for .pinterest.com/>]>\n"
     ]
    }
   ],
   "source": [
    "print(response.headers)\n",
    "print(response.cookies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7664940c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{'authenticated': True, 'user': 'user'}\n"
     ]
    }
   ],
   "source": [
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "response = requests.get('https://httpbin.org/basic-auth/user/pass',\n",
    "                        auth=HTTPBasicAuth('user', 'pass'))\n",
    "print(response.status_code)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7ef159",
   "metadata": {},
   "source": [
    " 1. requests.get()\n",
    "Used to get (fetch) data from a URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "240f28a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{\"current_user_url\":\"https://api.github.com/user\",\"current_user_authorizations_html_url\":\"https://github.com/settings/connections/applications{/client_id}\",\"authorizations_url\":\"https://api.github.com/authorizations\",\"code_search_url\":\"https://api.github.com/search/code?q={query}{&page,per_page,sort,order}\",\"commit_search_url\":\"https://api.github.com/search/commits?q={query}{&page,per_page,sort,order}\",\"emails_url\":\"https://api.github.com/user/emails\",\"emojis_url\":\"https://api.github.com/emojis\",\"events_url\":\"https://api.github.com/events\",\"feeds_url\":\"https://api.github.com/feeds\",\"followers_url\":\"https://api.github.com/user/followers\",\"following_url\":\"https://api.github.com/user/following{/target}\",\"gists_url\":\"https://api.github.com/gists{/gist_id}\",\"hub_url\":\"https://api.github.com/hub\",\"issue_search_url\":\"https://api.github.com/search/issues?q={query}{&page,per_page,sort,order}\",\"issues_url\":\"https://api.github.com/issues\",\"keys_url\":\"https://api.github.com/user/keys\",\"label_search_url\":\"https://api.github.com/search/labels?q={query}&repository_id={repository_id}{&page,per_page}\",\"notifications_url\":\"https://api.github.com/notifications\",\"organization_url\":\"https://api.github.com/orgs/{org}\",\"organization_repositories_url\":\"https://api.github.com/orgs/{org}/repos{?type,page,per_page,sort}\",\"organization_teams_url\":\"https://api.github.com/orgs/{org}/teams\",\"public_gists_url\":\"https://api.github.com/gists/public\",\"rate_limit_url\":\"https://api.github.com/rate_limit\",\"repository_url\":\"https://api.github.com/repos/{owner}/{repo}\",\"repository_search_url\":\"https://api.github.com/search/repositories?q={query}{&page,per_page,sort,order}\",\"current_user_repositories_url\":\"https://api.github.com/user/repos{?type,page,per_page,sort}\",\"starred_url\":\"https://api.github.com/user/starred{/owner}{/repo}\",\"starred_gists_url\":\"https://api.github.com/gists/starred\",\"topic_search_url\":\"https://api.github.com/search/topics?q={query}{&page,per_page}\",\"user_url\":\"https://api.github.com/users/{user}\",\"user_organizations_url\":\"https://api.github.com/user/orgs\",\"user_repositories_url\":\"https://api.github.com/users/{user}/repos{?type,page,per_page,sort}\",\"user_search_url\":\"https://api.github.com/search/users?q={query}{&page,per_page,sort,order}\"}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get('https://api.github.com')\n",
    "\n",
    "print(response.status_code)   # HTTP status code, like 200 (OK)\n",
    "print(response.text)          # Response content as a string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a21749",
   "metadata": {},
   "source": [
    "2. requests.post()\n",
    "Used to send data (like login info, forms) to a server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9155ec41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'pratham', 'age': 20}\n",
      "\n",
      "The connection is successful 200\n",
      "{'args': {}, 'data': '{\"name\": \"pratham\", \"age\": 20}', 'files': {}, 'form': {}, 'headers': {'Accept': '*/*', 'Accept-Encoding': 'gzip, deflate', 'Content-Length': '30', 'Content-Type': 'application/json', 'Host': 'httpbin.org', 'User-Agent': 'python-requests/2.32.3', 'X-Amzn-Trace-Id': 'Root=1-686cab0d-066e14214518729200d7c7b3'}, 'json': {'age': 20, 'name': 'pratham'}, 'origin': '103.87.29.96', 'url': 'https://httpbin.org/post'}\n"
     ]
    }
   ],
   "source": [
    "data = {\"name\":\"pratham\", \"age\": 20}\n",
    "response  = requests.post('https://httpbin.org/post', json=data)\n",
    "print(data)\n",
    "print()\n",
    "print(f'The connection is successful {response.status_code}' if response.status_code == 200 else 'failed')\n",
    "print(response.json()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80711004",
   "metadata": {},
   "source": [
    " 3. requests.put()\n",
    "Used to update existing data on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "724be7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{\n",
      "  \"args\": {}, \n",
      "  \"data\": \"\", \n",
      "  \"files\": {}, \n",
      "  \"form\": {\n",
      "    \"name\": \"Prathamesh Talele\"\n",
      "  }, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*\", \n",
      "    \"Accept-Encoding\": \"gzip, deflate\", \n",
      "    \"Content-Length\": \"22\", \n",
      "    \"Content-Type\": \"application/x-www-form-urlencoded\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"python-requests/2.32.3\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-686cab0f-0575f9725200275602c86f0e\"\n",
      "  }, \n",
      "  \"json\": null, \n",
      "  \"origin\": \"103.87.29.96\", \n",
      "  \"url\": \"https://httpbin.org/put\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = {'name': 'Prathamesh Talele'}\n",
    "\n",
    "response = requests.put('https://httpbin.org/put', data=data)\n",
    "\n",
    "print(response.status_code)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea2c57e",
   "metadata": {},
   "source": [
    "4. requests.delete()\n",
    "Used to delete data on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7b1c5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The connection is successful 200\n",
      "{\n",
      "  \"args\": {}, \n",
      "  \"data\": \"\", \n",
      "  \"files\": {}, \n",
      "  \"form\": {}, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*\", \n",
      "    \"Accept-Encoding\": \"gzip, deflate\", \n",
      "    \"Content-Length\": \"0\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"python-requests/2.32.3\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-686cab11-7b9071ad2a4c7dfe56253419\"\n",
      "  }, \n",
      "  \"json\": null, \n",
      "  \"origin\": \"103.87.29.96\", \n",
      "  \"url\": \"https://httpbin.org/delete\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "respone = requests.delete('https://httpbin.org/delete' )\n",
    "\n",
    "print(f'The connection is successful {respone.status_code}' if respone.status_code == 200 else \"failed\")\n",
    "print(respone.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8240c821",
   "metadata": {},
   "source": [
    " 5. requests.head()\n",
    "Sends a request just for the headers (not the content). Useful to check if a page exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9adf7d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Date': 'Tue, 08 Jul 2025 05:22:26 GMT', 'Content-Type': 'application/json', 'Content-Length': '306', 'Connection': 'keep-alive', 'Server': 'gunicorn/19.9.0', 'Access-Control-Allow-Origin': '*', 'Access-Control-Allow-Credentials': 'true'}\n"
     ]
    }
   ],
   "source": [
    "response = requests.head('https://httpbin.org/get')\n",
    "print(response.headers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b81425",
   "metadata": {},
   "source": [
    "6. requests.options()\n",
    "Used to find out what methods are allowed on a server (like GET, POST, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e6bfea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET, OPTIONS, HEAD\n"
     ]
    }
   ],
   "source": [
    "response = requests.options('https://httpbin.org')\n",
    "print(response.headers['Allow'])  # Example header, replace with the desired one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6ae21c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://httpbin.org/get?search=python\n"
     ]
    }
   ],
   "source": [
    "params = {'search': 'python'}\n",
    "\n",
    "response = requests.get('https://httpbin.org/get', params=params)\n",
    "\n",
    "print(response.url)  # See the full URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "158cb67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n",
      "{\n",
      "  \"args\": {}, \n",
      "  \"data\": \"\", \n",
      "  \"files\": {}, \n",
      "  \"form\": {\n",
      "    \"username\": \"pratham\"\n",
      "  }, \n",
      "  \"headers\": {\n",
      "    \"Accept\": \"*/*\", \n",
      "    \"Accept-Encoding\": \"gzip, deflate\", \n",
      "    \"Content-Length\": \"16\", \n",
      "    \"Content-Type\": \"application/x-www-form-urlencoded\", \n",
      "    \"Host\": \"httpbin.org\", \n",
      "    \"User-Agent\": \"myapp/1.0\", \n",
      "    \"X-Amzn-Trace-Id\": \"Root=1-686cab15-607ea7817df8e8d9442482e5\"\n",
      "  }, \n",
      "  \"json\": null, \n",
      "  \"origin\": \"103.87.29.96\", \n",
      "  \"url\": \"https://httpbin.org/post\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = \"https://httpbin.org/post\"\n",
    "headers = {\"User-Agent\": \"myapp/1.0\"}\n",
    "data = {\"username\": \"pratham\"}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=data)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Success!\")\n",
    "    print(response.text)\n",
    "else:\n",
    "    print(\"Failed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa61569b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Amazon.in for 'laptop'...\n",
      "\n",
      "Scraping: https://www.amazon.in/s?k=laptop&page=1\n",
      "Scraping: https://www.amazon.in/s?k=laptop&page=2\n",
      "Scraping: https://www.amazon.in/s?k=laptop&page=3\n",
      "Scraping: https://www.amazon.in/s?k=laptop&page=4\n",
      "Scraping: https://www.amazon.in/s?k=laptop&page=5\n",
      "Saved 104 products to amazon_laptops_advanced.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "# ---------- CONFIGURABLE PART ----------\n",
    "SEARCH_QUERY = \"laptop\"\n",
    "BASE_URL = \"https://www.amazon.in\"\n",
    "MAX_PAGES = 5  # Number of pages to scrape\n",
    "OUTPUT_FILE = \"amazon_laptops_advanced.csv\"\n",
    "\n",
    "HEADERS_LIST = [\n",
    "    {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\"\n",
    "    },\n",
    "    {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.8\"\n",
    "    },\n",
    "    {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64)\",\n",
    "        \"Accept-Language\": \"en-GB,en;q=0.7\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# ---------- FUNCTION TO SCRAPE ONE PAGE ----------\n",
    "def scrape_page(url):\n",
    "    print(f\"Scraping: {url}\")\n",
    "    headers = random.choice(HEADERS_LIST)\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch page: {url}, Status Code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "    items = soup.select(\"div.s-main-slot div[data-component-type='s-search-result']\")\n",
    "\n",
    "    product_list = []\n",
    "\n",
    "    for item in items:\n",
    "        title = item.h2.text.strip() if item.h2 else None\n",
    "        link = BASE_URL + item.h2.a['href'] if item.h2 and item.h2.a else None\n",
    "\n",
    "        price_whole = item.select_one(\"span.a-price-whole\")\n",
    "        price = price_whole.text.strip().replace(\",\", \"\") if price_whole else None\n",
    "\n",
    "        rating = item.select_one(\"span.a-icon-alt\")\n",
    "        rating = rating.text.strip() if rating else None\n",
    "\n",
    "        reviews = item.select_one(\"span.a-size-base\")\n",
    "        reviews = reviews.text.strip() if reviews else None\n",
    "\n",
    "        product = {\n",
    "            \"Title\": title,\n",
    "            \"Price (INR)\": price,\n",
    "            \"Rating\": rating,\n",
    "            \"Review Count\": reviews,\n",
    "            \"Product URL\": link\n",
    "        }\n",
    "        product_list.append(product)\n",
    "\n",
    "    return product_list\n",
    "\n",
    "# ---------- MAIN SCRAPER LOOP ----------\n",
    "def scrape_amazon(query, max_pages=1):\n",
    "    all_products = []\n",
    "    page = 1\n",
    "\n",
    "    while page <= max_pages:\n",
    "        search_url = f\"{BASE_URL}/s?k={quote_plus(query)}&page={page}\"\n",
    "        products = scrape_page(search_url)\n",
    "\n",
    "        if not products:\n",
    "            print(\"No products found or blocked. Stopping.\")\n",
    "            break\n",
    "\n",
    "        all_products.extend(products)\n",
    "\n",
    "        # Random sleep to avoid blocking\n",
    "        time.sleep(random.uniform(2, 5))\n",
    "        page += 1\n",
    "\n",
    "    return all_products\n",
    "\n",
    "# ---------- SAVE RESULTS ----------\n",
    "def save_to_csv(products, filename):\n",
    "    df = pd.DataFrame(products)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved {len(products)} products to {filename}\")\n",
    "\n",
    "# ---------- RUNNING THE SCRAPER ----------\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Scraping Amazon.in for '{SEARCH_QUERY}'...\\n\")\n",
    "    data = scrape_amazon(SEARCH_QUERY, MAX_PAGES)\n",
    "    save_to_csv(data, OUTPUT_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dae95788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Amazon.in for multiple categories...\n",
      "\n",
      "\n",
      "Scraping category: laptop\n",
      "Scraping: https://www.amazon.in/s?k=laptop&page=1\n",
      "Scraping: https://www.amazon.in/s?k=laptop&page=2\n",
      "Scraping: https://www.amazon.in/s?k=laptop&page=3\n",
      "\n",
      "Scraping category: smartphone\n",
      "Scraping: https://www.amazon.in/s?k=smartphone&page=1\n",
      "Scraping: https://www.amazon.in/s?k=smartphone&page=2\n",
      "Scraping: https://www.amazon.in/s?k=smartphone&page=3\n",
      "\n",
      "Scraping category: headphones\n",
      "Scraping: https://www.amazon.in/s?k=headphones&page=1\n",
      "Scraping: https://www.amazon.in/s?k=headphones&page=2\n",
      "Scraping: https://www.amazon.in/s?k=headphones&page=3\n",
      "\n",
      "Scraping category: books\n",
      "Scraping: https://www.amazon.in/s?k=books&page=1\n",
      "Scraping: https://www.amazon.in/s?k=books&page=2\n",
      "Scraping: https://www.amazon.in/s?k=books&page=3\n",
      "Saved 258 products to amazon_products.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "# ---------- CONFIGURABLE PART ----------\n",
    "SEARCH_QUERIES = [\"laptop\", \"smartphone\", \"headphones\", \"books\"]\n",
    "BASE_URL = \"https://www.amazon.in\"\n",
    "MAX_PAGES = 3  # Number of pages to scrape per category\n",
    "OUTPUT_FILE = \"amazon_products.csv\"\n",
    "\n",
    "HEADERS_LIST = [\n",
    "    {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\"\n",
    "    },\n",
    "    {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.8\"\n",
    "    },\n",
    "    {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64)\",\n",
    "        \"Accept-Language\": \"en-GB,en;q=0.7\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# ---------- FUNCTION TO SCRAPE ONE PAGE ----------\n",
    "def scrape_page(url, category):\n",
    "    print(f\"Scraping: {url}\")\n",
    "    headers = random.choice(HEADERS_LIST)\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch page: {url}, Status Code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "    items = soup.select(\"div.s-main-slot div[data-component-type='s-search-result']\")\n",
    "\n",
    "    product_list = []\n",
    "\n",
    "    for item in items:\n",
    "        title = item.h2.text.strip() if item.h2 else None\n",
    "        link = BASE_URL + item.h2.a['href'] if item.h2 and item.h2.a else None\n",
    "\n",
    "        price_whole = item.select_one(\"span.a-price-whole\")\n",
    "        price_fraction = item.select_one(\"span.a-price-fraction\")\n",
    "        price = f\"{price_whole.text.strip().replace(',', '')}.{price_fraction.text.strip()}\" if price_whole and price_fraction else None\n",
    "\n",
    "        rating = item.select_one(\"span.a-icon-alt\")\n",
    "        rating = rating.text.strip() if rating else None\n",
    "\n",
    "        reviews = item.select_one(\"span.a-size-base\")\n",
    "        reviews = reviews.text.strip() if reviews else None\n",
    "\n",
    "        badge = item.select_one(\"span.s-label-popover-default\")\n",
    "        badge_text = badge.text.strip() if badge else None\n",
    "\n",
    "        image = item.select_one(\"img.s-image\")\n",
    "        image_url = image[\"src\"] if image else None\n",
    "\n",
    "        brand = item.select_one(\"span.a-size-base-plus.a-color-base\")\n",
    "        brand_name = brand.text.strip() if brand else None\n",
    "\n",
    "        delivery = item.select_one(\"span.a-color-base.a-text-bold\")\n",
    "        delivery_info = delivery.text.strip() if delivery else None\n",
    "\n",
    "        discount = item.select_one(\"span.a-letter-space + span.a-size-base.a-color-secondary\")\n",
    "        discount_text = discount.text.strip() if discount else None\n",
    "\n",
    "        product = {\n",
    "            \"Category\": category,\n",
    "            \"Title\": title,\n",
    "            \"Brand\": brand_name,\n",
    "            \"Price (INR)\": price,\n",
    "            \"Rating\": rating,\n",
    "            \"Review Count\": reviews,\n",
    "            \"Product Badge\": badge_text,\n",
    "            \"Product URL\": link,\n",
    "            \"Image URL\": image_url,\n",
    "            \"Delivery Info\": delivery_info,\n",
    "            \"Discount\": discount_text\n",
    "        }\n",
    "        product_list.append(product)\n",
    "\n",
    "    return product_list\n",
    "\n",
    "# ---------- MAIN SCRAPER LOOP ----------\n",
    "def scrape_amazon(queries, max_pages=1):\n",
    "    all_products = []\n",
    "\n",
    "    for query in queries:\n",
    "        print(f\"\\nScraping category: {query}\")\n",
    "        page = 1\n",
    "\n",
    "        while page <= max_pages:\n",
    "            search_url = f\"{BASE_URL}/s?k={quote_plus(query)}&page={page}\"\n",
    "            products = scrape_page(search_url, query)\n",
    "\n",
    "            if not products:\n",
    "                print(\"No products found or blocked. Stopping.\")\n",
    "                break\n",
    "\n",
    "            all_products.extend(products)\n",
    "\n",
    "            time.sleep(random.uniform(2, 5))\n",
    "            page += 1\n",
    "\n",
    "    return all_products\n",
    "\n",
    "# ---------- SAVE RESULTS ----------\n",
    "def save_to_csv(products, filename):\n",
    "    df = pd.DataFrame(products)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved {len(products)} products to {filename}\")\n",
    "\n",
    "# ---------- RUNNING THE SCRAPER ----------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Scraping Amazon.in for multiple categories...\\n\")\n",
    "    data = scrape_amazon(SEARCH_QUERIES, MAX_PAGES)\n",
    "    save_to_csv(data, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16464bac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
